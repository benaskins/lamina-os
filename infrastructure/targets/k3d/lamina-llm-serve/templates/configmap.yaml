apiVersion: v1
kind: ConfigMap
metadata:
  name: {{ include "lamina-llm-serve.fullname" . }}-config
  namespace: {{ .Release.Namespace }}
  labels:
    {{- include "lamina-llm-serve.labels" . | nindent 4 }}
data:
  models.yaml: |
    models:
      llama3.2-1b-q4_k_m:
        path: "llama3.2-1b-q4_k_m/Llama-3.2-1B-Instruct-Q4_K_M.gguf"
        backend: "llama.cpp"
        size: "0.8GB"
        description: "Ultra-efficient 1B parameter model, ideal for quick responses"
        quantization: "Q4_K_M"
        use_cases: ["conversational", "testing"]
        download:
          source: "huggingface"
          repo_id: "bartowski/Llama-3.2-1B-Instruct-GGUF"
          filename: "Llama-3.2-1B-Instruct-Q4_K_M.gguf"

      llama3.2-3b-q4_k_m:
        path: "llama3.2-3b-q4_k_m/Llama-3.2-3B-Instruct-Q4_K_M.gguf"
        backend: "llama.cpp"
        size: "2.0GB"
        description: "Efficient 3B parameter model, good for general conversation"
        quantization: "Q4_K_M"
        use_cases: ["conversational", "reasoning"]
        download:
          source: "huggingface"
          repo_id: "bartowski/Llama-3.2-3B-Instruct-GGUF"
          filename: "Llama-3.2-3B-Instruct-Q4_K_M.gguf"

      test-model:
        path: "test-model/model.gguf"
        backend: "llama.cpp"
        size: "1.0GB"
        description: "Test model for regression testing"
        quantization: "Q4_K_M"
        use_cases: ["conversational", "testing"]

    categories:
      conversational: ["llama3.2-1b-q4_k_m", "llama3.2-3b-q4_k_m", "test-model"]
      testing: ["llama3.2-1b-q4_k_m", "test-model"]
      reasoning: ["llama3.2-3b-q4_k_m"]

    backends:
      llama.cpp:
        executable: "llama-server"
        args: ["--model", "{model_path}", "--port", "{port}"]
