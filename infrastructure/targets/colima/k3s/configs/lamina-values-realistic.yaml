# ðŸœ„ Lamina OS Production Values for M3 Ultra (512GB)
# Optimized for heavy AI workloads and enterprise-scale compute

global:
  environment: production
  sigil: "ðŸœ„"
  hardware_profile: m3_ultra_512gb

# Lamina Core Service - Realistic production scaling
lamina-core:
  image:
    repository: lamina-core
    tag: latest
    pullPolicy: IfNotPresent
  
  # Conservative replica count for shared environment
  replicaCount: 2
  
  # Reasonable resource allocation (sharing with Colima)
  resources:
    requests:
      memory: "4Gi"      # Reasonable base allocation
      cpu: "1"           # 1 core per instance
    limits:
      memory: "8Gi"      # Allow burst to 8GB
      cpu: "2"           # Max 2 cores per instance
  
  # Horizontal Pod Autoscaling for moderate scaling
  autoscaling:
    enabled: true
    minReplicas: 2
    maxReplicas: 6       # Scale to 6 instances max (48GB total)
    targetCPUUtilizationPercentage: 70
    targetMemoryUtilizationPercentage: 75
    # Custom metrics for AI workload scaling
    customMetrics:
      - type: Pods
        pods:
          metric:
            name: agent_requests_per_second
          target:
            type: AverageValue
            averageValue: "50"
  
  # High availability configuration
  podDisruptionBudget:
    enabled: true
    minAvailable: 2
  
  # Performance optimizations
  env:
    - name: GOMAXPROCS
      value: "4"
    - name: GOMEMLIMIT
      value: "15GiB"
    - name: AGENT_POOL_SIZE
      value: "20"         # Large agent pool
    - name: CONCURRENT_REQUESTS
      value: "100"        # Handle many concurrent requests
    - name: MEMORY_CACHE_SIZE
      value: "8GiB"       # Large in-memory cache

# Lamina LLM Serve - Realistic model serving
lamina-llm-serve:
  image:
    repository: lamina-llm-serve
    tag: latest
    pullPolicy: IfNotPresent
  
  # Conservative model serving replicas
  replicaCount: 2
  
  # Reasonable resource allocation for shared environment
  resources:
    requests:
      memory: "16Gi"     # Base allocation for medium models
      cpu: "2"           # 2 cores per model server
    limits:
      memory: "32Gi"     # Burst to 32GB for larger models
      cpu: "4"           # Max 4 cores per server
  
  # Autoscaling for model serving load
  autoscaling:
    enabled: true
    minReplicas: 1       # Start with 1 model server
    maxReplicas: 4       # Scale to 4 servers max (128GB total)
    targetCPUUtilizationPercentage: 80
    targetMemoryUtilizationPercentage: 85
    # Model-specific scaling metrics
    customMetrics:
      - type: Pods
        pods:
          metric:
            name: model_inference_queue_length
          target:
            type: AverageValue
            averageValue: "10"
      - type: Pods
        pods:
          metric:
            name: model_memory_utilization
          target:
            type: AverageValue
            averageValue: "80"
  
  # Persistent storage for model collection (realistic)
  persistence:
    enabled: true
    storageClass: "mac-ssd-fast"
    size: "200Gi"        # 200GB for model storage
    accessMode: ReadWriteOnce
    
  # Model-specific environment
  env:
    - name: MODEL_CACHE_SIZE
      value: "150Gi"     # 150GB model cache
    - name: MAX_CONCURRENT_MODELS
      value: "8"         # Serve 8 models simultaneously
    - name: MODEL_LOAD_TIMEOUT
      value: "600s"      # Extended timeout for large models
    - name: BATCH_SIZE
      value: "32"        # Large batch processing
    - name: GPU_MEMORY_FRACTION
      value: "0.9"       # Use most of available memory
    
  # Model preloading for performance
  modelPreload:
    enabled: true
    models:
      - name: "llama3.2-3b"
        priority: "high"
      - name: "llama3.2-11b" 
        priority: "high"
      - name: "mistral-7b"
        priority: "medium"
      - name: "codellama-13b"
        priority: "medium"

# ChromaDB - Realistic vector database for production
chromadb:
  image:
    repository: chromadb/chroma
    tag: latest
    pullPolicy: IfNotPresent
  
  # Single instance for development/testing (can cluster later)
  replicaCount: 1        # 1 ChromaDB instance
  
  # Reasonable resources for vector operations
  resources:
    requests:
      memory: "8Gi"      # Base allocation
      cpu: "1"           # 1 core per instance
    limits:
      memory: "16Gi"     # Can burst to 16GB
      cpu: "2"           # Max 2 cores per instance
  
  # Persistent storage for vector data
  persistence:
    enabled: true
    storageClass: "mac-ssd-fast"
    size: "100Gi"       # 100GB for vector storage
    accessMode: ReadWriteOnce
  
  # ChromaDB specific configuration
  config:
    chroma_db_impl: "clickhouse"  # Use ClickHouse backend for performance
    chroma_api_impl: "chromadb.api.fastapi.FastAPI"
    chroma_server_host: "0.0.0.0"
    chroma_server_http_port: 8000
    chroma_server_grpc_port: 50051
    
    # Performance tuning
    max_batch_size: 1000
    max_request_size_bytes: 104857600  # 100MB requests
    query_default_limit: 1000
    
  # High availability
  podDisruptionBudget:
    enabled: true
    minAvailable: 3      # Always keep 3/5 nodes available

# Storage configuration optimized for Mac SSD
storage:
  classes:
    mac-ssd-fast:
      provisioner: "rancher.io/local-path"
      parameters:
        basePath: "/opt/lamina/storage"
        mountOptions: "noatime,nodiratime,nodev,nosuid"
      reclaimPolicy: "Retain"
      volumeBindingMode: "WaitForFirstConsumer"
      allowVolumeExpansion: true

# Monitoring stack sized for enterprise workloads
monitoring:
  prometheus:
    enabled: true
    retention: "90d"     # Extended retention
    storageSize: "100Gi" # Large metrics storage
    resources:
      requests:
        memory: "8Gi"
        cpu: "2"
      limits:
        memory: "16Gi"
        cpu: "4"
    
    # High cardinality metrics for detailed monitoring
    scrapeConfigs:
      - job_name: 'lamina-detailed'
        scrape_interval: 15s
        metrics_path: '/metrics'
        params:
          level: ['detailed']
  
  grafana:
    enabled: true
    persistence:
      enabled: true
      size: "20Gi"
    resources:
      requests:
        memory: "2Gi"
        cpu: "1"
      limits:
        memory: "4Gi"
        cpu: "2"
  
  # Distributed tracing for complex workflows
  jaeger:
    enabled: true
    sampling_rate: 0.1   # 10% sampling for performance
    storage:
      type: "elasticsearch"
      elasticsearch:
        size: "100Gi"
        replicas: 3

# Network policies for production security
networkPolicies:
  enabled: true
  policies:
    # Lamina services can communicate with each other
    - name: lamina-internal
      podSelector:
        matchLabels:
          app.kubernetes.io/part-of: lamina
      policyTypes:
      - Ingress
      - Egress
      ingress:
      - from:
        - podSelector:
            matchLabels:
              app.kubernetes.io/part-of: lamina
      egress:
      - to:
        - podSelector:
            matchLabels:
              app.kubernetes.io/part-of: lamina
    
    # Monitoring can scrape all pods
    - name: monitoring-access
      podSelector:
        matchLabels:
          app.kubernetes.io/name: prometheus
      policyTypes:
      - Egress
      egress:
      - to: []

# Resource quotas to prevent runaway consumption
resourceQuotas:
  enabled: true
  quotas:
    compute:
      requests.cpu: "12"      # Reserve 12 cores (conservative)
      requests.memory: "200Gi" # Reserve 200GB (sharing with Colima)
      limits.cpu: "14"        # Max 14 cores
      limits.memory: "256Gi"  # Max 256GB
    storage:
      requests.storage: "300Gi" # Up to 300GB storage
      persistentvolumeclaims: "50"
    objects:
      pods: "200"
      services: "50"
      secrets: "100"
      configmaps: "100"

# Pod security policies for production hardening
podSecurityPolicy:
  enabled: true
  allowPrivilegeEscalation: false
  readOnlyRootFilesystem: true
  runAsNonRoot: true
  runAsUser: 10000
  fsGroup: 10000
  seLinuxOptions:
    level: "s0:c123,c456"
  seccompProfile:
    type: RuntimeDefault
  capabilities:
    drop:
    - ALL
    add:
    - NET_BIND_SERVICE

# Ingress configuration for production access
ingress:
  enabled: true
  className: "nginx"
  annotations:
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    nginx.ingress.kubernetes.io/use-regex: "true"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
  
  hosts:
    - host: lamina.local
      paths:
        - path: /api/v1/
          pathType: Prefix
          service: lamina-core
        - path: /models/
          pathType: Prefix  
          service: lamina-llm-serve
        - path: /vector/
          pathType: Prefix
          service: chromadb
  
  tls:
    - secretName: lamina-tls
      hosts:
        - lamina.local